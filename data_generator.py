'''
data generator generates the binary data based on the available weights of
the network which need to be finetuned.

One main goal of this class is to generate more data samples in order to
get rid of the float outputs of the neural network.

For example, if we have classifier output of 0.7, we may generate 100 different samples
based on p = 0.7. By sampling, we can transfer the float output to binary data samples by
adding more data samples. But then the subsequently training would be much easier.
'''

from sklearn import preprocessing
import numpy as np
from utils_mpf import load, save
import gzip
import pickle


class data_generator(object):

    def __init__(self, data_dict, n_samples):
        '''
        :param nets: nets is the network which needs to be finetuned, it always contains the weights and bias term
        :param data_dict: is of the form { 'layer_#': layer output file path}
        :param n_samples: the number of samples generated by sampling
        '''

        # if self.W is None:
        #     self.W = nets.W
        #     self.bias = nets.bias

        if self.data_dict is None:
            self.data_dict = data_dict

        self.n_samples = n_samples

    def data_binarize(self, mnist = True, label_vectors = None):
        '''

        :param mnist: whether we are using mnist datasets.
        if we are using mnist, we are starting from the first layer of the whole network, and the first layer
        need to be binarized, while the other layers need to be sampled.

        :return: data_full_layer is one instantiation of the binary outputs of all the neurons
        '''


        binarizer = preprocessing.Binarizer(threshold=0.5)
        data_layer_1 =  binarizer.transform(load(self.data_dict['layer_1']))


        list_keys = list(self.data_dict.keys())
        num_layer= len(list_keys)

        data_full_layer = None


        for i in range(len(list_keys)):
            k = 'layer_' + str(i+1)
            v = self.data_dict[k]
            if k == 'layer_1':
                data_full_layer = data_layer_1
            elif k != 'layer_' + str(num_layer):
                activation = np.random.binomial(1,load(v))
                data_full_layer = np.concatenate((data_full_layer,activation), axis = 1)
            else:
                data_full_layer = np.concatenate((data_full_layer,label_vectors), axis = 1)

        return data_full_layer

    def data_generator(self,mnist = True):

        '''
        :param minst: whether the data set is mnist
        :return: n_sample instantiations of the binary outputs for all neurons
        '''
        data_samples = None

        f = gzip.open('mnist.pkl.gz', 'rb')
        train_set, valid_set, test_set = pickle.load(f,encoding="bytes")
        train_label = train_set[1]
        f.close()
        label_vectors = generate_label_vector(train_label, num_classes=10)

        for i in range(self.n_samples):
            if data_samples is None:
                    data_samples = self.data_binarize(mnist= mnist, label_vectors= label_vectors)
            else:
                    data_samples = np.concatenate((data_samples,self.data_binarize(mnist = mnist)),axis = 0)

        data_path = 'binary_data_samples.npy'
        np.save(data_path, data_samples)

        return data_path


def generate_label_vector(label, num_classes):
    a = []
    for i in range(len(label)):
        b = np.zeros(num_classes)
        b[label[i]] = 1
        a.append(b)

    return np.asarray(a)











